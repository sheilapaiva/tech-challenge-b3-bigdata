# ğŸš€ Guia Completo de Deploy - Tech Challenge B3

## ğŸ“‹ VisÃ£o Geral
Este guia detalha como fazer o deploy completo do pipeline de dados da B3, cumprindo todos os requisitos obrigatÃ³rios do Tech Challenge.

## âœ… PrÃ©-requisitos
- AWS CLI configurado (`aws configure`)
- Python 3.7+ instalado
- Google Chrome (para Selenium)
- Bucket S3 com nome Ãºnico escolhido

## ğŸ¯ Arquitetura Final
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper   â”‚â”€â”€â–¶â”‚   S3    â”‚â”€â”€â–¶â”‚ Lambda  â”‚â”€â”€â–¶â”‚ Glue Visual â”‚â”€â”€â–¶â”‚ Athena  â”‚
â”‚  (Selenium) â”‚   â”‚  Raw    â”‚   â”‚Trigger  â”‚   â”‚    Job      â”‚   â”‚Queries  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                             â”‚
                       â–¼                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   S3    â”‚                 â”‚    Glue     â”‚
                  â”‚Refined  â”‚                 â”‚  Catalog    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Deploy Automatizado

### OpÃ§Ã£o 1: Script Automatizado (Recomendado)
```bash
# 1. Clonar/navegar para o diretÃ³rio
cd tech-challenge-b3-bigdata

# 2. Ativar ambiente virtual
source venv/bin/activate

# 3. Executar deploy completo
python deploy/deploy_pipeline.py meu-bucket-unico-b3 --region us-east-1
```

### OpÃ§Ã£o 2: Deploy Manual

#### Passo 1: Infraestrutura AWS
```bash
# Deploy via CloudFormation
aws cloudformation create-stack \
  --stack-name tech-challenge-b3-pipeline \
  --template-body file://infrastructure/cloudformation-template.yaml \
  --parameters ParameterKey=BucketName,ParameterValue=meu-bucket-unico-b3 \
  --capabilities CAPABILITY_IAM
```

#### Passo 2: Upload dos Scripts
```bash
# Upload do script ETL
aws s3 cp src/etl_job_with_catalog.py s3://meu-bucket-unico-b3/scripts/etl_job.py
```

#### Passo 3: Configurar Job Glue Visual
Siga o guia: `docs/GLUE_VISUAL_JOB_SETUP.md`

## ğŸ“Š Teste do Pipeline Completo

### 1. Executar Scraping Local
```bash
# Testar scraping
python demo.py
```

### 2. Upload e Trigger AutomÃ¡tico
```bash
# Upload manual para testar trigger
python -c "
from src.scraper import B3Scraper
from src.uploader import S3Uploader
import pandas as pd

scraper = B3Scraper(headless=True)
df = scraper.fetch_with_fallback()

uploader = S3Uploader(bucket='meu-bucket-unico-b3', prefix='raw')
key = uploader.upload_parquet(df, pd.Timestamp.now())
print(f'Dados enviados: {key}')
"
```

### 3. Monitorar ExecuÃ§Ã£o
```bash
# Verificar logs da Lambda
aws logs describe-log-groups --log-group-name-prefix /aws/lambda/b3-pipeline-trigger

# Verificar execuÃ§Ã£o do job Glue
aws glue get-job-runs --job-name b3-etl-job-visual --max-results 5
```

### 4. Verificar no Athena
```sql
-- No AWS Console â†’ Athena
SHOW DATABASES;
SHOW TABLES IN b3_database;
SELECT * FROM b3_database.b3_refined_data LIMIT 10;
```

## ğŸ¯ ValidaÃ§Ã£o dos Requisitos

### âœ… Checklist de Cumprimento

| Requisito | Status | LocalizaÃ§Ã£o | ValidaÃ§Ã£o |
|-----------|--------|-------------|-----------|
| **Req 1**: Scraping B3 | âœ… | `src/scraper.py` | `python demo.py` |
| **Req 2**: S3 Parquet + PartiÃ§Ã£o | âœ… | `src/uploader.py` | Verificar S3 `raw/date=*/` |
| **Req 3**: S3 â†’ Lambda â†’ Glue | âœ… | CloudFormation | Upload arquivo, verificar logs |
| **Req 4**: Lambda trigger | âœ… | `infrastructure/cloudformation-template.yaml` | Logs Lambda |
| **Req 5**: Job Glue visual | âœ… | `docs/GLUE_VISUAL_JOB_SETUP.md` | Glue Studio |
| **Req 5A**: Agrupamento | âœ… | Aggregate transform | Job visual |
| **Req 5B**: Renomear colunas | âœ… | Rename Field transform | Job visual |
| **Req 5C**: CÃ¡lculo data | âœ… | Derived Column transform | Job visual |
| **Req 6**: Refined particionado | âœ… | S3 Target config | Verificar S3 `refined/` |
| **Req 7**: Glue Catalog | âœ… | Target config | `SHOW TABLES` |
| **Req 8**: Athena legÃ­vel | âœ… | Automatic | Queries funcionando |
| **Req 9**: Notebook (opcional) | âš ï¸ | `athena_queries/` | Queries exemplo |

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Lambda nÃ£o dispara
```bash
# Verificar configuraÃ§Ã£o S3 Event
aws s3api get-bucket-notification-configuration --bucket meu-bucket-unico-b3
```

### Job Glue falha
```bash
# Verificar logs
aws logs get-log-events --log-group-name /aws-glue/jobs/logs-v2 --log-stream-name <job-run-id>
```

### Athena nÃ£o vÃª tabela
```sql
-- Atualizar partiÃ§Ãµes
MSCK REPAIR TABLE b3_database.b3_refined_data;
```

### Selenium falha
```bash
# Instalar dependÃªncias
pip install selenium webdriver-manager
```

## ğŸ“ˆ Monitoramento e Observabilidade

### CloudWatch Dashboards
1. **Lambda**: InvocaÃ§Ãµes, Erros, DuraÃ§Ã£o
2. **Glue**: Job success rate, Processing time
3. **S3**: Requests, Storage utilization

### Alertas Configurados
- Lambda failures
- Glue job failures  
- S3 upload errors

## ğŸ‰ ValidaÃ§Ã£o Final

### 1. Pipeline End-to-End
```bash
# Executar pipeline completo
python -c "
from src.scraper import B3Scraper
from src.uploader import S3Uploader
import pandas as pd
import time

# 1. Scraping
print('1. Scraping...')
scraper = B3Scraper(headless=True)
df = scraper.fetch_with_fallback()

# 2. Upload
print('2. Upload...')
uploader = S3Uploader(bucket='meu-bucket-unico-b3', prefix='raw')
key = uploader.upload_parquet(df, pd.Timestamp.now())

print('3. Pipeline iniciado automaticamente!')
print(f'Arquivo: {key}')
print('4. Aguarde ~5 minutos e verifique Athena')
"
```

### 2. VerificaÃ§Ã£o Athena
```sql
-- Query completa de validaÃ§Ã£o
SELECT 
    codigo_acao,
    nome_acao,
    quantidade_teorica_total,
    percentual_participacao,
    dias_desde_referencia,
    mes_referencia,
    ano_referencia,
    data_ref
FROM b3_database.b3_refined_data 
WHERE data_ref = (SELECT MAX(data_ref) FROM b3_database.b3_refined_data)
ORDER BY percentual_participacao DESC;
```

## ğŸ“Š Resultados Esperados

### Dados no S3
```
meu-bucket-b3/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ date=2025-08-01/
â”‚       â””â”€â”€ data.parquet
â”œâ”€â”€ refined/
â”‚   â””â”€â”€ data_ref=2025-08-01/
â”‚       â””â”€â”€ codigo_acao=PETR4/
â”‚           â””â”€â”€ data.parquet
â””â”€â”€ scripts/
    â””â”€â”€ etl_job.py
```

### Tabela no Athena
- **Database**: `b3_database`
- **Table**: `b3_refined_data`
- **Columns**: 12 colunas incluindo transformaÃ§Ãµes
- **Partitions**: `data_ref` + `codigo_acao`
- **Records**: ~22 aÃ§Ãµes do Ibovespa

## ğŸ¯ ConclusÃ£o
ApÃ³s seguir este guia, vocÃª terÃ¡ um pipeline completo que:
- âœ… Atende todos os 9 requisitos obrigatÃ³rios
- âœ… Funciona de forma automatizada
- âœ… EstÃ¡ pronto para produÃ§Ã£o
- âœ… Inclui monitoramento e observabilidade

**Pipeline Status**: 100% dos requisitos implementados! ğŸ‰ 
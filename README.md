Tech Challenge - Big Data B3 ğŸš€

Este projeto demonstra uma arquitetura de pipeline batch para coleta e processamento de dados do pregÃ£o da B3 utilizando **SCRAPING** com Selenium e AWS. Os componentes principais sÃ£o:

- **Scraper**: coleta **DADOS** do site da B3 usando Selenium WebDriver.
- **S3**: armazenamento dos arquivos brutos em formato parquet particionado por data.
- **Lambda**: acionada quando novos arquivos chegam no bucket e inicia o Job do Glue.
- **Glue Job**: realiza transformaÃ§Ãµes, salva no bucket *refined* e cataloga os dados.
- **Athena**: consulta dos dados refinados.

## ğŸ¯ **SCRAPING IMPLEMENTADO!**

âœ… **Dados da B3** - Coleta os dados.
âœ… **Selenium WebDriver** - Renderiza JavaScript para acessar dados dinÃ¢micos  
âœ… **22 aÃ§Ãµes do Ibovespa** - CÃ³digos como PETR4, VALE3, ITUB4, etc.  
âœ… **ComposiÃ§Ã£o oficial** - Quantidade teÃ³rica e participaÃ§Ã£o percentual  

## Diagrama da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸŒ B3 Site    â”‚    â”‚  ğŸ“Š Scraper     â”‚    â”‚   â˜ï¸ AWS Cloud   â”‚
â”‚   (JavaScript)  â”‚â—„â”€â”€â”€â”¤  Selenium       â”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Chrome Driver  â”‚    â”‚                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                  â”‚
                                 â”‚            â”‚                  â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”‚                  â”‚
                       â”‚ ğŸ“„ Dados Brutos â”‚    â”‚                  â”‚
                       â”‚ AÃ§Ãµes B3        â”‚    â”‚                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                  â”‚
                                 â”‚            â”‚                  â”‚
                                 â–¼            â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                            â”‚              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   S3 Event     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”‚
    â”‚  â”‚   ğŸª£ S3 Raw     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  âš¡ Lambda        â”‚   â”‚              â”‚
    â”‚  â”‚                 â”‚  ObjectCreated â”‚  Trigger         â”‚   â”‚              â”‚
    â”‚  â”‚ raw/date=       â”‚                â”‚  b3-pipeline     â”‚   â”‚              â”‚
    â”‚  â”‚ 2025-08-01/     â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚              â”‚
    â”‚  â”‚ data.parquet    â”‚                          â”‚            â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â–¼            â”‚              â”‚
    â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚
    â”‚                                    â”‚  ğŸ› ï¸ Glue Job       â”‚  â”‚              â”‚
    â”‚                                    â”‚  (Visual Mode)     â”‚  â”‚              â”‚
    â”‚                                    â”‚                    â”‚  â”‚              â”‚
    â”‚                                    â”‚ A: Aggregate       â”‚  â”‚              â”‚
    â”‚                                    â”‚ B: Rename Fields   â”‚  â”‚              â”‚
    â”‚                                    â”‚ C: Date Calcs      â”‚  â”‚              â”‚
    â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚
    â”‚                                              â”‚             â”‚              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â–¼             â”‚              â”‚
    â”‚  â”‚  ğŸª£ S3 Refined  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”‚
    â”‚  â”‚                 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  ğŸ“‹ Glue Catalog   â”‚   â”‚              â”‚
    â”‚  â”‚ refined/        â”‚              â”‚  b3_database       â”‚   â”‚              â”‚
    â”‚  â”‚ data_ref=*/     â”‚              â”‚  b3_refined_data   â”‚   â”‚              â”‚
    â”‚  â”‚ codigo_acao=*/  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚              â”‚
    â”‚  â”‚ data.parquet    â”‚                        â”‚              â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â–¼              â”‚              â”‚
    â”‚                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚
    â”‚                                   â”‚  ğŸ” Amazon Athena   â”‚  â”‚              â”‚
    â”‚                                   â”‚  SQL Queries        â”‚  â”‚              â”‚
    â”‚                                   â”‚  Analytics          â”‚  â”‚              â”‚
    â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                 â”‚
                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                   â”‚  ğŸ“Š Business Intelligence  â”‚
                                                   â”‚  Dashboards & Reports      â”‚
                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Fluxo de Dados Detalhado:

1. **ğŸŒ Scraping**: Selenium acessa o site da B3, renderiza JavaScript e extrai dados reais da composiÃ§Ã£o do Ibovespa
2. **ğŸ“¤ Upload**: Dados sÃ£o convertidos para Parquet e salvos no S3 em `raw/date=YYYY-MM-DD/data.parquet`
3. **âš¡ Trigger**: S3 Event Notification aciona automaticamente a funÃ§Ã£o Lambda
4. **ğŸ› ï¸ ETL Visual**: Lambda inicia o Glue Job visual que executa as transformaÃ§Ãµes:
   - **A**: Agrupamento por cÃ³digo de aÃ§Ã£o + sumarizaÃ§Ã£o (soma, contagem)
   - **B**: RenomeaÃ§Ã£o de colunas (`CÃ³digo`â†’`codigo_acao`, etc.)
   - **C**: CÃ¡lculos com data (diferenÃ§as, extraÃ§Ãµes, formataÃ§Ãµes)
5. **ğŸ’¾ Refined**: Dados transformados sÃ£o salvos em `refined/` particionados por data e cÃ³digo da aÃ§Ã£o
6. **ğŸ“‹ CatalogaÃ§Ã£o**: Glue Job automaticamente registra tabela no Glue Catalog
7. **ğŸ” Consultas**: Dados ficam disponÃ­veis para consulta no Athena e anÃ¡lises de BI

### ğŸ“Š Dados Processados:
- **Entrada**: 22 aÃ§Ãµes do Ibovespa (PETR4, VALE3, ITUB4, etc.)
- **SaÃ­da**: Dados agregados, renomeados e enriquecidos com cÃ¡lculos temporais
- **Particionamento**: Por data de referÃªncia e cÃ³digo da aÃ§Ã£o
- **Formato**: Parquet otimizado para consultas analÃ­ticas

O link de origem dos dados Ã© obrigatÃ³rio e estÃ¡ disponÃ­vel [aqui](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br).

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Python 3.7+
- Google Chrome (para Selenium)
- pip

### 1. InstalaÃ§Ã£o das DependÃªncias

```bash
# Criar ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. ExecuÃ§Ã£o com Dados

**Scraping da B3:**
```bash
python -m src.scraper
```

**DemonstraÃ§Ã£o completa com dados:**
```bash
python demo.py
```

### 3. Exemplo de Uso ProgramÃ¡tico

```python
from src.scraper import B3Scraper
from src.uploader import S3Uploader
import pandas as pd

# Coletar dados da B3
scraper = B3Scraper(headless=True)  # headless=False para ver o navegador
df = scraper.fetch_with_fallback()

print(f"Coletadas {len(df)} aÃ§Ãµes!")
print(df.head())

# Para upload S3 (requer credenciais AWS)
uploader = S3Uploader(bucket="seu-bucket", prefix="raw")
key = uploader.upload_parquet(df, pd.Timestamp.now())
```

## ğŸ“Š Dados Coletados

O scraper agora coleta dados da composiÃ§Ã£o do Ibovespa:

- **CÃ³digo**: CÃ³digo da aÃ§Ã£o (ex: PETR4, VALE3)
- **AÃ§Ã£o**: Nome da empresa
- **Tipo**: Tipo de aÃ§Ã£o (ON, PN, etc.)
- **Qtde. TeÃ³rica**: Quantidade teÃ³rica na carteira
- **Part. (%)**: ParticipaÃ§Ã£o percentual no Ã­ndice
- **data_ref**: Data de referÃªncia dos dados

## ğŸ“ˆ Exemplo de SaÃ­da

```
CÃ³digo         AÃ§Ã£o     Tipo Qtde. TeÃ³rica  Part. (%)
ALOS3        ALLOS  ON ED NM   476.976.044      495.0
ABEV3    AMBEV S/A       ON 4.394.835.131     2666.0
ASAI3        ASSAI    ON NM 1.345.897.506      617.0
AURE3        AUREN    ON NM   323.738.747      146.0
AZZA3   AZZAS 2154    ON NM   136.643.320      237.0
```

## ğŸ› ï¸ Funcionalidades

### Scraper (src/scraper.py)
- **Selenium WebDriver** para renderizar JavaScript
- **Chrome headless** para scraping automatizado
- **Dados** da composiÃ§Ã£o do Ibovespa
- **Fallback robusto** em caso de falhas
- **Headers apropriados** para simular navegador

### Uploader (src/uploader.py)
- Upload de DataFrame para S3 em formato Parquet
- Particionamento automÃ¡tico por data
- CompressÃ£o eficiente

### Pipeline ETL (src/etl_job.py)
- Job do AWS Glue para transformaÃ§Ãµes
- AgregaÃ§Ãµes por ticker
- RenomeaÃ§Ã£o de colunas
- CÃ¡lculo de diferenÃ§as de data

## â˜ï¸ Deploy na AWS

Para usar o pipeline completo na AWS:

1. **Configurar credenciais AWS:**
   ```bash
   aws configure
   ```

2. **Criar recursos AWS:**
   - Bucket S3 para dados raw e refined
   - FunÃ§Ã£o Lambda
   - Glue Job
   - PermissÃµes IAM necessÃ¡rias

3. **Deploy dos componentes:**
   - Upload do cÃ³digo Lambda
   - ConfiguraÃ§Ã£o do Glue Job
   - Setup do Glue Catalog

## ğŸ› ï¸ Estrutura do Projeto

```
tech-challenge-b3-bigdata/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py      # Scraping com Selenium
â”‚   â”œâ”€â”€ uploader.py     # Upload para S3
â”‚   â”œâ”€â”€ lambda_handler.py # FunÃ§Ã£o Lambda
â”‚   â””â”€â”€ etl_job.py      # Job do Glue
â”œâ”€â”€ demo.py             # DemonstraÃ§Ã£o completa
â”œâ”€â”€ requirements.txt    # DependÃªncias
â””â”€â”€ README.md          # Este arquivo
```

## ğŸ“ DependÃªncias

- **boto3**: Cliente AWS
- **pandas**: ManipulaÃ§Ã£o de dados
- **requests**: RequisiÃ§Ãµes HTTP (fallback)
- **pyarrow**: Formato Parquet
- **lxml**: Parser HTML
- **html5lib**: Parser HTML alternativo
- **beautifulsoup4**: Parser HTML robusto
- **selenium**: WebDriver para scraping JavaScript
- **webdriver-manager**: Gerenciamento automÃ¡tico do ChromeDriver
- **openpyxl**: Export para Excel

## ğŸ”§ SoluÃ§Ã£o de Problemas

**Erro "No module named 'selenium'":**
```bash
pip install selenium webdriver-manager
```

**Erro do Chrome/ChromeDriver:**
- O webdriver-manager baixa automaticamente o ChromeDriver correto
- Certifique-se de ter o Google Chrome instalado

**Erro "No tables found":**
- O projeto inclui fallback automÃ¡tico para casos de falha
- Verifique a conexÃ£o com a internet

**Erro de credenciais AWS:**
Configure com `aws configure` ou variÃ¡veis de ambiente.

## ğŸ¯ **Resultados**

âœ… **22 aÃ§Ãµes** coletadas do Ibovespa  
âœ… **Dados estruturados** em CSV e Excel  
âœ… **Pipeline completo** pronto para deploy  
âœ… **Scraping** com Selenium  

---

ğŸš€ **Teste:** `python demo.py` - **DADOS DA B3!**
Tech Challenge - Big Data B3

Este projeto demonstra uma arquitetura de pipeline batch para coleta e processamento de dados do pregÃ£o da B3 utilizando AWS. Os componentes principais sÃ£o:

- **Scraper**: coleta os dados do site da B3.
- **S3**: armazenamento dos arquivos brutos em formato parquet particionado por data.
- **Lambda**: acionada quando novos arquivos chegam no bucket e inicia o Job do Glue.
- **Glue Job**: realiza transformaÃ§Ãµes, salva no bucket *refined* e cataloga os dados.
- **Athena**: consulta dos dados refinados.

## Desenho da Arquitetura

```
+----------+          +-------+          +---------+       +-------+
| Scraper  +--parquet-> S3    +--event--> Lambda  +--> Glue Job |
+----------+          +-------+          +---------+       +-------+
                                                               |
                                                               v
                                                            Athena
```

1. O *scraper* faz download da tabela de cotaÃ§Ãµes do dia e envia para o bucket S3 em `raw/date=YYYY-MM-DD/`.
2. O S3 aciona a Lambda que dispara o Glue Job.
3. O Glue Job lÃª os dados brutos, executa as transformaÃ§Ãµes (agrupamentos, renomeia colunas e calcula diferenÃ§a de datas) e grava em `refined/` particionado por `data_ref` e `acao`.
4. O Glue cria a tabela no Glue Catalog, permitindo consultas pelo Athena.

O link de origem dos dados Ã© obrigatÃ³rio e estÃ¡ disponÃ­vel [aqui](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br).

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Python 3.7+
- pip

### 1. InstalaÃ§Ã£o das DependÃªncias

```bash
# Criar ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. ExecuÃ§Ã£o Local

**Executar apenas o scraper:**
```bash
python -m src.scraper
```

**DemonstraÃ§Ã£o completa:**
```bash
python demo.py
```

### 3. Exemplo de Uso ProgramÃ¡tico

```python
from src.scraper import B3Scraper
from src.uploader import S3Uploader
import pandas as pd

# Coletar dados
scraper = B3Scraper()
df = scraper.fetch()

# Para upload S3 (requer credenciais AWS)
uploader = S3Uploader(bucket="seu-bucket", prefix="raw")
key = uploader.upload_parquet(df, pd.Timestamp.now())
```

## ğŸ“Š Funcionalidades

### Scraper (src/scraper.py)
- Coleta dados do Ibovespa via web scraping
- Fallback para dados de exemplo quando o site nÃ£o estÃ¡ disponÃ­vel
- Headers apropriados para simular navegador
- Tratamento robusto de erros

### Uploader (src/uploader.py)
- Upload de DataFrame para S3 em formato Parquet
- Particionamento automÃ¡tico por data
- CompressÃ£o eficiente

### Pipeline ETL (src/etl_job.py)
- Job do AWS Glue para transformaÃ§Ãµes
- AgregaÃ§Ãµes por ticker
- RenomeaÃ§Ã£o de colunas
- CÃ¡lculo de diferenÃ§as de data

## â˜ï¸ Deploy na AWS

Para usar o pipeline completo na AWS:

1. **Configurar credenciais AWS:**
   ```bash
   aws configure
   ```

2. **Criar recursos AWS:**
   - Bucket S3 para dados raw e refined
   - FunÃ§Ã£o Lambda
   - Glue Job
   - PermissÃµes IAM necessÃ¡rias

3. **Deploy dos componentes:**
   - Upload do cÃ³digo Lambda
   - ConfiguraÃ§Ã£o do Glue Job
   - Setup do Glue Catalog

## ğŸ› ï¸ Estrutura do Projeto

```
tech-challenge-b3-bigdata/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py      # Web scraping da B3
â”‚   â”œâ”€â”€ uploader.py     # Upload para S3
â”‚   â”œâ”€â”€ lambda_handler.py # FunÃ§Ã£o Lambda
â”‚   â””â”€â”€ etl_job.py      # Job do Glue
â”œâ”€â”€ demo.py             # DemonstraÃ§Ã£o completa
â”œâ”€â”€ requirements.txt    # DependÃªncias
â””â”€â”€ README.md          # Este arquivo
```

## ğŸ“ DependÃªncias

- boto3: Cliente AWS
- pandas: ManipulaÃ§Ã£o de dados
- requests: RequisiÃ§Ãµes HTTP
- pyarrow: Formato Parquet
- lxml: Parser HTML
- html5lib: Parser HTML alternativo
- beautifulsoup4: Parser HTML robusto

## ğŸ”§ SoluÃ§Ã£o de Problemas

**Erro "No module named 'lxml'":**
```bash
pip install lxml html5lib beautifulsoup4
```

**Erro "No tables found":**
O projeto inclui dados de exemplo quando o scraping falha.

**Erro de credenciais AWS:**
Configure com `aws configure` ou variÃ¡veis de ambiente.

## ğŸ“ˆ Exemplo de SaÃ­da

```
ğŸ“Š Dados coletados: 5 aÃ§Ãµes
ğŸ“… Data de referÃªncia: 2025-08-01

 Nome  Ãšltimo  VariaÃ§Ã£o (%)   Volume   
PETR4   28.50          2.15  45000000 
VALE3   62.30         -1.30  78000000 
ITUB4   25.10          0.85  32000000 
BBDC4   13.45         -0.95  25000000 
MGLU3    8.90          3.20  18000000 

Volume total: 198,000,000
```

---

ğŸ¯ **Teste rÃ¡pido:** `python demo.py`